{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms #  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Linear Regression##\n",
    "\n",
    "1. Loss function: RSS $L = \\sum{(\\hat{y}_{i} - y_{i})^2}$ \n",
    "\n",
    "    Note: different from total sum of square $L = \\sum{(y_{i} - \\bar{y})^2}$\n",
    "\n",
    "2. Improvements: \n",
    "\n",
    "    i. Feature selection\n",
    "    \n",
    "    ii. Dimention reduction(PCA)\n",
    "    \n",
    "    iii. Regularization(Lasso/Ridge/Elastic Net)\n",
    "    - Which one is better, Lasso/Ridge? [Quora answer by Eric Msechu](https://www.quora.com/When-is-Ridge-regression-favorable-over-Lasso-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Soft Margin SVM (classification)\n",
    "\n",
    "1. Loss function: min $W^{T}W+ C\\sum_{i = 1}^{n}\\xi_{i}$ s.t. $ \\forall i$ $y_{i}(W^{T}x_{i}+b)\\ge 1 - \\xi_{i}$ $\\forall i \\ge 0$\n",
    "\n",
    "2. Larger C means strict, thus soft margin will become hard margin\n",
    "\n",
    "3. Kernalized SVM: [nice videos about mathematics behind SVM](https://www.youtube.com/user/joshstarmer/videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Random Forest (bagging, non-sequencial)\n",
    "\n",
    "1. Algorithm:\n",
    "\n",
    "    i. Sample m datasets from D with replacement\n",
    "    \n",
    "    ii. For each subset, train a decision tree while only considering k random features (where $k \\lt d$)\n",
    "\n",
    "2. Prediction(Mode for classification, mean for regression)\n",
    "\n",
    "3. Pros:\n",
    "\n",
    "    i. reduce variance(compare to decision tree)\n",
    "    \n",
    "    ii. run efficiently on large datasets\n",
    "\n",
    "4. Cons:\n",
    "\n",
    "    i. hard to interpret\n",
    "    \n",
    "    ii. not great if feature are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.Boosting (sequencial)##\n",
    "note: differences between [bootstrap aggregating/bagging](https://www.youtube.com/watch?v=2Mg8QD0F1dQ) and [boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw)\n",
    "\n",
    "1. Gradient Boosting [StatQuest video](https://www.youtube.com/watch?v=3CC4N4z3GJc)\n",
    "\n",
    "2. AdaBoost\n",
    "\n",
    "3. XGBoost\n",
    "\n",
    "    i. Sequential tree growing\n",
    "    \n",
    "    ii. Minimizing loss function using Gradient descent\n",
    "    \n",
    "    iii. Parallel processing to increase speed\n",
    "    \n",
    "    iv. Regularization paremeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
